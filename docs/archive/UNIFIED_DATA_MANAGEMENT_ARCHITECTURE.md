# ç»Ÿä¸€æ•°æ®ç®¡ç†æ¶æ„æ–¹æ¡ˆ

## ğŸ“‹ é—®é¢˜æ ¹æºåˆ†æ

### å½“å‰é—®é¢˜
æ‚¨é‡åˆ°çš„"ä»Šæ—¥é¢„æµ‹æ¨¡å—åˆæ²¡æœ‰æ•°æ®"é—®é¢˜æ˜¯ç³»ç»Ÿæ€§çš„,æ ¹æœ¬åŸå› :

1. **æ•°æ®æºåˆ†æ•£**: å„æ¨¡å—ç‹¬ç«‹è°ƒç”¨AkShare/TuShare,æ²¡æœ‰ç»Ÿä¸€ç®¡ç†
2. **APIä¸ç¨³å®š**: AkShare APIé¢‘ç¹è¶…æ—¶,ç¼ºå°‘é™çº§ç­–ç•¥
3. **ç¼ºå°‘ç¼“å­˜**: é‡å¤è°ƒç”¨ç›¸åŒAPI,æµªè´¹èµ„æºä¸”ä¸ç¨³å®š
4. **æ•°æ®éªŒè¯ç¼ºå¤±**: æ²¡æœ‰æ£€æŸ¥æ•°æ®æ˜¯å¦æœ‰æ•ˆ(ç©ºæ•°æ®ã€è¿‡æœŸæ•°æ®)
5. **å‰åç«¯ä¸åŒ¹é…**: å‰ç«¯è°ƒç”¨çš„APIåœ¨ä»£ç æ¸…ç†æ—¶è¢«åˆ é™¤

### å…·ä½“è¡¨ç°
```
âŒ ä»Šæ—¥é¢„æµ‹: è°ƒç”¨ /api/time-segmented/predictions â†’ 404 (APIå·²åˆ é™¤)
âŒ å¸‚åœºæ•è·: è°ƒç”¨ /api/capture/* â†’ 404 (APIå·²åˆ é™¤)
âš ï¸  æ¿å—çƒ­åº¦: AkShareæ¿å—APIè¶…æ—¶ â†’ é™çº§åˆ°çƒ­é—¨è‚¡ç¥¨èšåˆ
âœ… æ¶¨åœé¢„æµ‹: /api/limit-up/predictions â†’ æœ‰æ•°æ®(ä½†å¯èƒ½ä¸ç¨³å®š)
```

---

## ğŸ¯ ç»Ÿä¸€æ•°æ®ç®¡ç†æ¶æ„

### æ•´ä½“è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    å‰ç«¯ç»„ä»¶å±‚                             â”‚
â”‚  ä»Šæ—¥é¢„æµ‹ â”‚ æ¿å—çƒ­åº¦ â”‚ å¸‚åœºæ‰«æ â”‚ äºŒæ¿å€™é€‰ â”‚ æœºä¼šæµ      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ç»Ÿä¸€æ•°æ®æœåŠ¡å±‚ (Data Service Layer)          â”‚
â”‚                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  æ¶¨åœé¢„æµ‹   â”‚  â”‚  æ¿å—çƒ­åº¦   â”‚  â”‚  å¸‚åœºæ‰«æ   â”‚      â”‚
â”‚  â”‚   Service   â”‚  â”‚   Service   â”‚  â”‚   Service   â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚         â”‚                 â”‚                 â”‚              â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                           â”‚                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ç»Ÿä¸€æ•°æ®æºç®¡ç†å™¨ (Unified Data Source)         â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  ç¼“å­˜å±‚      â”‚  â”‚  é™çº§ç­–ç•¥    â”‚  â”‚  æ•°æ®éªŒè¯    â”‚    â”‚
â”‚  â”‚  Redis/å†…å­˜  â”‚  â”‚  3å±‚fallback â”‚  â”‚  å®Œæ•´æ€§æ£€æŸ¥  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  é‡è¯•æœºåˆ¶    â”‚  â”‚  å¥åº·æ£€æŸ¥    â”‚  â”‚  æ•°æ®åˆ·æ–°    â”‚    â”‚
â”‚  â”‚  æŒ‡æ•°é€€é¿    â”‚  â”‚  APIå¯ç”¨æ€§   â”‚  â”‚  è‡ªåŠ¨æ›´æ–°    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   å¤–éƒ¨æ•°æ®æº                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚AkShare â”‚  â”‚TuShare â”‚  â”‚ä¸œæ–¹è´¢å¯Œâ”‚  â”‚æ–°æµªè´¢ç»â”‚           â”‚
â”‚  â”‚ ä¸»åŠ›   â”‚  â”‚ å¤‡ç”¨   â”‚  â”‚ è¡¥å……   â”‚  â”‚ è¡¥å……   â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—ï¸ å®æ–½æ–¹æ¡ˆ

### ç¬¬ä¸€é˜¶æ®µ: ç»Ÿä¸€æ•°æ®æºç®¡ç†å™¨ (æœ¬å‘¨)

#### 1. åˆ›å»ºUnifiedDataSourceåŸºç±»

**æ–‡ä»¶**: `backend/core/unified_data_source.py`

```python
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional, List
from datetime import datetime, timedelta
import asyncio
import logging

logger = logging.getLogger(__name__)

class DataSourceStatus:
    """æ•°æ®æºçŠ¶æ€"""
    def __init__(self):
        self.is_healthy = True
        self.last_success = None
        self.last_error = None
        self.error_count = 0
        self.total_requests = 0
        self.success_rate = 1.0

class UnifiedDataSource(ABC):
    """ç»Ÿä¸€æ•°æ®æºåŸºç±»"""

    def __init__(self):
        self.cache = {}  # ç®€å•å†…å­˜ç¼“å­˜
        self.status = DataSourceStatus()
        self.cache_ttl = 60  # ç¼“å­˜60ç§’

    async def get_data(
        self,
        data_type: str,
        params: Dict[str, Any] = None,
        use_cache: bool = True
    ) -> Dict[str, Any]:
        """
        è·å–æ•°æ®çš„ç»Ÿä¸€å…¥å£

        Args:
            data_type: æ•°æ®ç±»å‹ (limit_up, hot_stocks, sectorsç­‰)
            params: æŸ¥è¯¢å‚æ•°
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜

        Returns:
            æ ‡å‡†åŒ–æ•°æ®å“åº”
        """
        # 1. æ£€æŸ¥ç¼“å­˜
        if use_cache:
            cached = self._get_from_cache(data_type, params)
            if cached:
                logger.info(f"âœ… ç¼“å­˜å‘½ä¸­: {data_type}")
                return cached

        # 2. é™çº§ç­–ç•¥: ä¸»æ•°æ®æº â†’ å¤‡ç”¨æº â†’ æ¨¡æ‹Ÿæ•°æ®
        for source_name, source_func in self._get_sources(data_type):
            try:
                logger.info(f"ğŸ” å°è¯•æ•°æ®æº: {source_name}")
                data = await self._fetch_with_retry(source_func, params)

                # 3. æ•°æ®éªŒè¯
                if self._validate_data(data):
                    # 4. æ›´æ–°ç¼“å­˜
                    self._save_to_cache(data_type, params, data)
                    # 5. æ›´æ–°å¥åº·çŠ¶æ€
                    self._update_status(success=True)
                    logger.info(f"âœ… {source_name} è·å–æˆåŠŸ")
                    return data
                else:
                    logger.warning(f"âš ï¸  {source_name} æ•°æ®éªŒè¯å¤±è´¥")

            except Exception as e:
                logger.warning(f"âŒ {source_name} å¤±è´¥: {e}")
                self._update_status(success=False, error=str(e))
                continue

        # 6. æ‰€æœ‰æ•°æ®æºéƒ½å¤±è´¥,è¿”å›é™çº§æ•°æ®
        logger.error(f"âŒ æ‰€æœ‰æ•°æ®æºå¤±è´¥,è¿”å›é™çº§æ•°æ®: {data_type}")
        return self._get_fallback_data(data_type, params)

    async def _fetch_with_retry(
        self,
        fetch_func,
        params: Dict,
        max_retries: int = 3
    ) -> Dict[str, Any]:
        """å¸¦é‡è¯•çš„æ•°æ®è·å–"""
        for i in range(max_retries):
            try:
                return await fetch_func(params)
            except Exception as e:
                if i == max_retries - 1:
                    raise
                wait_time = 2 ** i  # æŒ‡æ•°é€€é¿
                logger.warning(f"â³ é‡è¯• {i+1}/{max_retries}, ç­‰å¾…{wait_time}ç§’...")
                await asyncio.sleep(wait_time)

    def _validate_data(self, data: Dict[str, Any]) -> bool:
        """æ•°æ®æœ‰æ•ˆæ€§éªŒè¯"""
        if not data:
            return False

        # æ£€æŸ¥æ•°æ®ç»“æ„
        if 'code' not in data or data['code'] != 200:
            return False

        if 'data' not in data:
            return False

        # æ£€æŸ¥æ•°æ®ä¸ä¸ºç©º
        data_content = data['data']
        if isinstance(data_content, dict):
            if 'stocks' in data_content:
                return len(data_content['stocks']) > 0
            if 'sectors' in data_content:
                return len(data_content['sectors']) > 0

        return True

    def _get_from_cache(
        self,
        data_type: str,
        params: Dict
    ) -> Optional[Dict[str, Any]]:
        """ä»ç¼“å­˜è·å–æ•°æ®"""
        cache_key = self._make_cache_key(data_type, params)
        if cache_key in self.cache:
            cached_data, cached_time = self.cache[cache_key]
            if datetime.now() - cached_time < timedelta(seconds=self.cache_ttl):
                return cached_data
        return None

    def _save_to_cache(
        self,
        data_type: str,
        params: Dict,
        data: Dict[str, Any]
    ):
        """ä¿å­˜åˆ°ç¼“å­˜"""
        cache_key = self._make_cache_key(data_type, params)
        self.cache[cache_key] = (data, datetime.now())

    def _make_cache_key(self, data_type: str, params: Dict) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        import hashlib
        import json
        params_str = json.dumps(params or {}, sort_keys=True)
        return f"{data_type}:{hashlib.md5(params_str.encode()).hexdigest()}"

    def _update_status(self, success: bool, error: str = None):
        """æ›´æ–°æ•°æ®æºå¥åº·çŠ¶æ€"""
        self.status.total_requests += 1
        if success:
            self.status.last_success = datetime.now()
            self.status.is_healthy = True
            self.status.error_count = 0
        else:
            self.status.last_error = error
            self.status.error_count += 1
            # è¿ç»­å¤±è´¥3æ¬¡æ ‡è®°ä¸ºä¸å¥åº·
            if self.status.error_count >= 3:
                self.status.is_healthy = False

        # æ›´æ–°æˆåŠŸç‡
        if self.status.total_requests > 0:
            success_count = self.status.total_requests - self.status.error_count
            self.status.success_rate = success_count / self.status.total_requests

    @abstractmethod
    def _get_sources(self, data_type: str) -> List[tuple]:
        """è·å–æ•°æ®æºåˆ—è¡¨(éœ€å­ç±»å®ç°)"""
        pass

    @abstractmethod
    def _get_fallback_data(self, data_type: str, params: Dict) -> Dict[str, Any]:
        """è·å–é™çº§æ•°æ®(éœ€å­ç±»å®ç°)"""
        pass
```

#### 2. å®ç°å…·ä½“æ•°æ®æº

**æ–‡ä»¶**: `backend/core/stock_data_source.py`

```python
from .unified_data_source import UnifiedDataSource
import akshare as ak
import asyncio

class StockDataSource(UnifiedDataSource):
    """è‚¡ç¥¨æ•°æ®æº"""

    def __init__(self):
        super().__init__()
        self.cache_ttl = 30  # è‚¡ç¥¨æ•°æ®30ç§’ç¼“å­˜

    def _get_sources(self, data_type: str) -> List[tuple]:
        """å®šä¹‰æ•°æ®æºä¼˜å…ˆçº§"""
        sources = {
            'limit_up': [
                ('AkShareæ¶¨åœæ¦œ', self._fetch_limit_up_akshare),
                ('ä¸œæ–¹è´¢å¯Œ', self._fetch_limit_up_eastmoney),
            ],
            'hot_stocks': [
                ('AkShareçƒ­é—¨æ¦œ', self._fetch_hot_stocks_akshare),
                ('æ¨¡æ‹Ÿæ•°æ®', self._fetch_hot_stocks_mock),
            ],
            'sectors': [
                ('AkShareæ¿å—', self._fetch_sectors_akshare),
                ('çƒ­é—¨è‚¡ç¥¨èšåˆ', self._fetch_sectors_from_hot_stocks),
                ('æ¨¡æ‹Ÿæ•°æ®', self._fetch_sectors_mock),
            ],
        }
        return sources.get(data_type, [])

    async def _fetch_limit_up_akshare(self, params: Dict) -> Dict[str, Any]:
        """ä»AkShareè·å–æ¶¨åœæ•°æ®"""
        loop = asyncio.get_event_loop()
        df = await loop.run_in_executor(None, ak.stock_zt_pool_em, params.get('date'))

        stocks = []
        for _, row in df.iterrows():
            stocks.append({
                'code': row['ä»£ç '],
                'name': row['åç§°'],
                'price': row['æœ€æ–°ä»·'],
                'change_percent': row['æ¶¨è·Œå¹…'],
                'seal_time': row.get('å°æ¿æ—¶é—´', ''),
                'data_source': 'akshare'
            })

        return {
            'code': 200,
            'data': {
                'stocks': stocks,
                'count': len(stocks),
                'data_source': 'akshare'
            }
        }

    def _get_fallback_data(self, data_type: str, params: Dict) -> Dict[str, Any]:
        """é™çº§æ•°æ®"""
        return {
            'code': 200,
            'message': 'fallback data',
            'data': {
                'stocks': [],
                'count': 0,
                'data_source': 'fallback',
                'warning': 'æ•°æ®æºæš‚æ—¶ä¸å¯ç”¨,æ˜¾ç¤ºç©ºæ•°æ®'
            }
        }
```

---

### ç¬¬äºŒé˜¶æ®µ: æ•°æ®è´¨é‡ç›‘æ§ (ä¸‹å‘¨)

#### 1. æ•°æ®è´¨é‡æŒ‡æ ‡

```python
class DataQualityMetrics:
    """æ•°æ®è´¨é‡æŒ‡æ ‡"""

    def __init__(self):
        self.metrics = {
            'completeness': 0.0,  # å®Œæ•´æ€§: æœ‰æ•°æ®çš„æ¯”ä¾‹
            'freshness': 0.0,      # æ–°é²œåº¦: æ•°æ®æ›´æ–°æ—¶é—´
            'accuracy': 0.0,       # å‡†ç¡®æ€§: æ•°æ®éªŒè¯é€šè¿‡ç‡
            'availability': 0.0,   # å¯ç”¨æ€§: APIæˆåŠŸç‡
        }

    def calculate_completeness(self, data: Dict) -> float:
        """è®¡ç®—æ•°æ®å®Œæ•´æ€§"""
        if not data or 'data' not in data:
            return 0.0

        stocks = data['data'].get('stocks', [])
        if not stocks:
            return 0.0

        # æ£€æŸ¥å…³é”®å­—æ®µå®Œæ•´æ€§
        required_fields = ['code', 'name', 'price', 'change_percent']
        complete_count = 0

        for stock in stocks:
            if all(field in stock and stock[field] for field in required_fields):
                complete_count += 1

        return complete_count / len(stocks) if stocks else 0.0

    def calculate_freshness(self, data: Dict) -> float:
        """è®¡ç®—æ•°æ®æ–°é²œåº¦ (0-1,è¶Šæ¥è¿‘1è¶Šæ–°é²œ)"""
        if 'updated_at' not in data.get('data', {}):
            return 0.5  # æœªçŸ¥æ—¶é—´

        updated_at = datetime.fromisoformat(data['data']['updated_at'])
        age_seconds = (datetime.now() - updated_at).total_seconds()

        # 60ç§’å†…: 1.0, 5åˆ†é’Ÿ: 0.5, 10åˆ†é’Ÿä»¥ä¸Š: 0.0
        if age_seconds < 60:
            return 1.0
        elif age_seconds < 300:
            return 1.0 - (age_seconds - 60) / 240 * 0.5
        elif age_seconds < 600:
            return 0.5 - (age_seconds - 300) / 300 * 0.5
        else:
            return 0.0
```

#### 2. ç›‘æ§ä»ªè¡¨æ¿API

```python
@app.get("/api/monitoring/data-quality")
async def get_data_quality():
    """æ•°æ®è´¨é‡ç›‘æ§æ¥å£"""
    return {
        'limit_up': {
            'availability': 0.95,
            'completeness': 1.0,
            'freshness': 0.8,
            'last_update': '2025-10-02T14:30:00',
            'data_source': 'akshare',
            'health': 'healthy'
        },
        'sectors': {
            'availability': 0.60,
            'completeness': 0.85,
            'freshness': 0.9,
            'last_update': '2025-10-02T14:29:00',
            'data_source': 'hot_stocks_aggregation',
            'health': 'degraded'
        },
        'hot_stocks': {
            'availability': 0.90,
            'completeness': 1.0,
            'freshness': 1.0,
            'last_update': '2025-10-02T14:30:00',
            'data_source': 'akshare',
            'health': 'healthy'
        }
    }
```

---

### ç¬¬ä¸‰é˜¶æ®µ: è‡ªåŠ¨æ•°æ®åˆ·æ–° (ä¸‹å‘¨)

#### æ•°æ®åˆ·æ–°è°ƒåº¦å™¨

```python
from apscheduler.schedulers.asyncio import AsyncIOScheduler

class DataRefreshScheduler:
    """æ•°æ®åˆ·æ–°è°ƒåº¦å™¨"""

    def __init__(self, data_source: UnifiedDataSource):
        self.data_source = data_source
        self.scheduler = AsyncIOScheduler()

    def start(self):
        """å¯åŠ¨è°ƒåº¦ä»»åŠ¡"""
        # æ¯30ç§’åˆ·æ–°çƒ­é—¨è‚¡ç¥¨
        self.scheduler.add_job(
            self._refresh_hot_stocks,
            'interval',
            seconds=30,
            id='refresh_hot_stocks'
        )

        # æ¯5åˆ†é’Ÿåˆ·æ–°æ¿å—æ•°æ®
        self.scheduler.add_job(
            self._refresh_sectors,
            'interval',
            minutes=5,
            id='refresh_sectors'
        )

        # äº¤æ˜“æ—¶é—´å†…æ¯1åˆ†é’Ÿåˆ·æ–°æ¶¨åœæ•°æ®
        self.scheduler.add_job(
            self._refresh_limit_up,
            'cron',
            hour='9-15',
            minute='*/1',
            id='refresh_limit_up'
        )

        self.scheduler.start()

    async def _refresh_hot_stocks(self):
        """åˆ·æ–°çƒ­é—¨è‚¡ç¥¨"""
        await self.data_source.get_data('hot_stocks', use_cache=False)

    async def _refresh_sectors(self):
        """åˆ·æ–°æ¿å—æ•°æ®"""
        await self.data_source.get_data('sectors', use_cache=False)

    async def _refresh_limit_up(self):
        """åˆ·æ–°æ¶¨åœæ•°æ®"""
        await self.data_source.get_data('limit_up', use_cache=False)
```

---

## ğŸš€ ç«‹å³è¡ŒåŠ¨: ä¿®å¤ä»Šæ—¥é¢„æµ‹

### çŸ­æœŸæ–¹æ¡ˆ (ä»Šå¤©)

**ç›´æ¥å°†æ—¶é—´åˆ†å±‚é¢„æµ‹APIè¿ç§»åˆ°LimitUpModule**:

```python
# backend/modules/limit_up/module.py

@self.router.get("/predictions/time-segmented")
async def get_time_segmented_predictions(limit: int = 100):
    """æ—¶é—´åˆ†å±‚æ¶¨åœé¢„æµ‹"""
    return await self.service.get_time_segmented_predictions(limit)
```

### å‰ç«¯æ›´æ–°

```typescript
// frontend/src/components/TimeLayeredLimitUpTracker.tsx

// ä¿®æ”¹APIè°ƒç”¨
const response = await fetch(
  `http://localhost:9000/api/limit-up/predictions/time-segmented?limit=100`
);
```

---

## ğŸ“Š æ•°æ®ç®¡ç†æœ€ä½³å®è·µ

### 1. ç»Ÿä¸€æ•°æ®æ ¼å¼

æ‰€æœ‰APIè¿”å›ç»Ÿä¸€æ ¼å¼:

```json
{
  "code": 200,
  "message": "success",
  "data": {
    "stocks": [...],
    "count": 10,
    "updated_at": "2025-10-02T14:30:00",
    "data_source": "akshare",
    "cache_hit": false,
    "quality_score": 0.95
  },
  "meta": {
    "api_version": "2.0",
    "request_id": "abc123",
    "processing_time": 0.5
  }
}
```

### 2. æ•°æ®æºä¼˜å…ˆçº§

```
ä¼˜å…ˆçº§1: AkShare (ä¸»åŠ›æ•°æ®æº)
ä¼˜å…ˆçº§2: ä¸œæ–¹è´¢å¯ŒAPI (å¤‡ç”¨)
ä¼˜å…ˆçº§3: çƒ­é—¨è‚¡ç¥¨èšåˆ (é™çº§)
ä¼˜å…ˆçº§4: æ¨¡æ‹Ÿæ•°æ® (å…œåº•)
```

### 3. ç¼“å­˜ç­–ç•¥

| æ•°æ®ç±»å‹ | ç¼“å­˜æ—¶é•¿ | åˆ·æ–°ç­–ç•¥ |
|---------|---------|---------|
| å®æ—¶è¡Œæƒ… | 10ç§’ | äº¤æ˜“æ—¶é—´æ¯10ç§’ |
| æ¶¨åœæ¦œå• | 30ç§’ | äº¤æ˜“æ—¶é—´æ¯30ç§’ |
| æ¿å—çƒ­åº¦ | 5åˆ†é’Ÿ | æ¯5åˆ†é’Ÿ |
| å†å²æ•°æ® | 1å°æ—¶ | æ¯å°æ—¶ |

### 4. é”™è¯¯å¤„ç†

```python
# ç»Ÿä¸€é”™è¯¯å“åº”
{
  "code": 500,
  "message": "æ•°æ®è·å–å¤±è´¥",
  "error": {
    "type": "DataSourceError",
    "details": "AkShare APIè¶…æ—¶",
    "retry_after": 60,
    "fallback_available": true
  }
}
```

---

## ğŸ¯ é¢„æœŸæ•ˆæœ

å®æ–½ç»Ÿä¸€æ•°æ®ç®¡ç†å:

âœ… **ç¨³å®šæ€§**: 99%+ APIå¯ç”¨ç‡ (å¤šæ•°æ®æºé™çº§)
âœ… **å®æ—¶æ€§**: æ•°æ®å»¶è¿Ÿ < 30ç§’
âœ… **å‡†ç¡®æ€§**: æ•°æ®å®Œæ•´æ€§ > 95%
âœ… **å¯ç»´æŠ¤æ€§**: ç»Ÿä¸€ç®¡ç†,æ˜“äºè°ƒè¯•
âœ… **å¯æ‰©å±•æ€§**: æ–°å¢æ•°æ®æºåªéœ€å®ç°æ¥å£

---

**åˆ›å»ºæ—¶é—´**: 2025-10-02
**ä¼˜å…ˆçº§**: P0 (æœ€é«˜)
**çŠ¶æ€**: æ–¹æ¡ˆåˆ¶å®šå®Œæˆ,å¾…å®æ–½
