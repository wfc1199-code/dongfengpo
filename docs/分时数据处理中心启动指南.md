# 分时数据处理中心启动指南

本文档示例如何在开发环境中依次启动重构后的核心服务，便于快速验证端到端链路。以下假设已安装 Python 3.11+，并在项目根目录执行。

## 1. 准备 Redis

```bash
brew install redis        # macOS 示例
redis-server --daemonize yes
```

确认 Redis 监听端口：默认 `6379`。

## 2. 创建虚拟环境

```bash
python -m venv venv
source venv/bin/activate
pip install --upgrade pip
```

## 3. 安装依赖（按服务拆分）

```bash
pip install -r services/collector-gateway/requirements.txt
pip install -r services/data-cleaner/requirements.txt
pip install -r services/data-lake-writer/requirements.txt
pip install -r services/feature-pipeline/requirements.txt
pip install -r services/strategy-engine/requirements.txt
pip install -r services/opportunity-aggregator/requirements.txt
pip install -r services/risk-guard/requirements.txt
pip install -r services/signal-api/requirements.txt
pip install -r services/signal-streamer/requirements.txt
pip install -r services/backtest-service/requirements.txt
```

如需一次性安装，可将上述文件合并到自定义 requirements。

## 4. 启动顺序

建议在多终端（或 tmux/session manager）中按顺序启动：

1. **数据采集链路**
   ```bash
   python -m collector_gateway.main
   python -m stream_buffer.main
   python -m data_cleaner.main
   python -m data_lake_writer.main
   ```
2. **特征与策略**
   ```bash
   python -m feature_pipeline.main
   python -m strategy_engine.main
   ```
3. **机会与风险**
   ```bash
   python -m opportunity_aggregator.main
   python -m risk_guard.main
   ```
4. **对外接口**
   ```bash
   python -m signal_api.main      # 默认 http://localhost:8000
   python -m signal_streamer.main # 默认 ws://localhost:8100/ws/opportunities
   python -m backtest_service.main # 默认 http://localhost:8200
   ```

所有服务均支持环境变量覆盖配置，例如：

```bash
export COLLECTOR_REDIS_URL="redis://127.0.0.1:6379/0"
export STRATEGY_ENGINE_STRATEGIES='[{"name":"rapid-rise","module":"strategy_engine.strategies.rapid_rise","class_name":"RapidRiseStrategy","parameters":{"min_change":2.5}}]'
```

## 5. 快速脚本

若已安装所有依赖，可使用脚本一次启动主要服务：

```bash
chmod +x scripts/start_pipeline.sh
./scripts/start_pipeline.sh
```

脚本会输出各服务 PID 并在根目录生成 `pipeline-services.pid`。按 `Ctrl+C` 可终止所有进程，脚本会自动清理 PID 文件。

## 6. 使用 Docker Compose

仓库已更新 `docker-compose.yml`，可在容器中启动各服务：

```bash
# 启动 Redis + 数据流水线核心服务
docker compose up redis collector-gateway stream-buffer data-cleaner \
  data-lake-writer feature-pipeline strategy-engine opportunity-aggregator \
  risk-guard signal-api signal-streamer backtest-service

# 如需附带 legacy 前后端
docker compose up backend frontend
```

所有服务共享源码卷，修改代码后可直接热重载（若需要更快的启动，可自定义镜像）。

## 7. 验证步骤

1. `curl http://localhost:8000/health` → 应返回 `{ "status": "ok" }`。
2. `curl http://localhost:8000/opportunities` → 如流水线有数据，返回机会列表。
3. 使用浏览器或工具连接 `ws://localhost:8100/ws/opportunities`，应实时收到机会推送。
4. `curl -X POST http://localhost:8200/backtests -d '{...}'` → 返回回测结果。

## 8. 常见问题

| 问题 | 排查建议 |
| --- | --- |
| Redis 连接失败 | 检查 `*_REDIS_URL` 是否与实际地址一致。 |
| 无机会数据 | 确认采集、清洗、特征、策略链路均已启动；可检查各服务日志。 |
| WebSocket 无推送 | 确认 `opportunity-aggregator` 是否发布到 `dfp:opportunities:ws` 频道。 |
| 回测数据不足 | 在 `data-lake` 目录放入 `<symbol>.parquet`，或使用内置模拟数据。 |

## 9. 关闭服务

在各终端使用 `Ctrl+C` 终止；确认 `redis-server` 可继续运行或手动停止：

```bash
redis-cli shutdown
```
