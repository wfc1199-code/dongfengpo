"""
æ¶¨åœæ¿ / è¿æ¿è¿½è¸ª API
ä½¿ç”¨ AkShare stock_zt_pool_em API è·å–å‡†ç¡®çš„è¿æ¿æ•°æ®
"""
from fastapi import APIRouter
import asyncio
import logging
from typing import Dict, Any
from datetime import datetime, timedelta

router = APIRouter(
    prefix="/api/limit-up",
    tags=["limit-up"],
    responses={404: {"description": "Not found"}},
)

logger = logging.getLogger(__name__)


@router.get("/predictions")
async def get_limit_up_predictions(limit: int = 50):
    """
    è·å–æ¶¨åœæ± æ•°æ®ï¼ˆåŒ…å«å‡†ç¡®çš„è¿æ¿æ•°ï¼‰
    
    ä½¿ç”¨ AkShare çš„ stock_zt_pool_em APIï¼Œä¸åŸå§‹åç«¯ä¸€è‡´
    è¿”å›åŒ…å« consecutive_days çš„è‚¡ç¥¨åˆ—è¡¨
    """
    try:
        import akshare as ak
        
        logger.info("ğŸ” è·å–æ¶¨åœæ± æ•°æ®ï¼ˆAkShare APIï¼‰...")
        
        loop = asyncio.get_event_loop()
        df = None
        data_date = None
        
        # å°è¯•è·å–æ¶¨åœæ•°æ®ï¼šå…ˆå°è¯•ä»Šå¤©ï¼Œå¦‚æœä¸ºç©ºåˆ™å°è¯•å‰å‡ ä¸ªäº¤æ˜“æ—¥
        for days_ago in range(5):
            try_date = datetime.now() - timedelta(days=days_ago)
            date_str = try_date.strftime("%Y%m%d")
            
            # è·³è¿‡å‘¨æœ«
            if try_date.weekday() >= 5:
                continue
            
            try:
                df = await loop.run_in_executor(
                    None,
                    ak.stock_zt_pool_em,
                    date_str
                )
                
                if df is not None and not df.empty:
                    data_date = date_str
                    if days_ago == 0:
                        logger.info(f"âœ… è·å–åˆ° {len(df)} åªæ¶¨åœè‚¡ç¥¨ - ä»Šæ—¥æ•°æ®")
                    else:
                        logger.info(f"âœ… è·å–åˆ° {len(df)} åªæ¶¨åœè‚¡ç¥¨ - {date_str}æ•°æ®")
                    break
                else:
                    logger.debug(f"{date_str} æ— æ¶¨åœæ•°æ®ï¼Œå°è¯•å‰ä¸€å¤©")
                    
            except Exception as e:
                logger.debug(f"è·å– {date_str} æ¶¨åœæ•°æ®å¤±è´¥: {e}")
                continue
        
        if df is None or df.empty:
            logger.warning("æ¶¨åœæ± æ•°æ®ä¸ºç©ºï¼ˆåŒ…æ‹¬å†å²æ•°æ®ï¼‰ï¼Œè¿”å›ç©ºåˆ—è¡¨")
            return {
                "code": 200,
                "message": "æš‚æ— æ¶¨åœæ•°æ®",
                "data": {"stocks": []},
                "date": None
            }
        
        # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        stocks = []
        for _, row in df.head(limit * 2).iterrows():  # å–æ›´å¤šä»¥ä¾¿æ’åºåç­›é€‰
            try:
                code = str(row.get('ä»£ç ', ''))
                name = str(row.get('åç§°', ''))
                price = float(row.get('æœ€æ–°ä»·', 0) or 0)
                change_percent = float(row.get('æ¶¨è·Œå¹…', 0) or 0)
                volume = int(row.get('æˆäº¤é‡', 0) or 0)
                amount = float(row.get('æˆäº¤é¢', 0) or 0)
                turnover_rate = float(row.get('æ¢æ‰‹ç‡', 0) or 0)
                consecutive_days = int(row.get('è¿æ¿æ•°', 1) or 1)  # AkShare ç›´æ¥è¿”å›è¿æ¿æ•°
                industry = str(row.get('æ‰€å±è¡Œä¸š', 'å…¶ä»–') or 'å…¶ä»–')
                
                # å°æ¿æ—¶é—´
                first_limit_time = str(row.get('é¦–æ¬¡å°æ¿æ—¶é—´', '') or '')
                last_limit_time = str(row.get('æœ€åå°æ¿æ—¶é—´', '') or '')
                
                stocks.append({
                    "symbol": code,
                    "code": code,  # å…¼å®¹å‰ç«¯
                    "name": name,
                    "price": price,
                    "change_percent": change_percent,
                    "consecutive_days": consecutive_days,
                    "industry": industry,
                    "volume": volume,
                    "amount": amount,
                    "turnover_rate": turnover_rate,
                    "first_limit_up_time": first_limit_time,
                    "last_limit_up_time": last_limit_time,
                    "reason": f"{consecutive_days}è¿æ¿" if consecutive_days > 1 else "é¦–æ¿æ¶¨åœ",
                    "data_source": "akshare_limit_up_pool"
                })
            except Exception as e:
                logger.warning(f"è½¬æ¢æ¶¨åœè‚¡ç¥¨æ•°æ®å¤±è´¥: {e}")
                continue
        
        # æŒ‰è¿æ¿æ•°æ’åºï¼ˆé«˜åˆ°ä½ï¼‰
        stocks.sort(key=lambda x: x['consecutive_days'], reverse=True)
        stocks = stocks[:limit]
        
        logger.info(f"âœ… è¿”å› {len(stocks)} åªæ¶¨åœè‚¡ç¥¨ï¼ˆåŒ…å«å‡†ç¡®è¿æ¿æ•°ï¼‰")
        
        # ç»Ÿè®¡è¿æ¿åˆ†å¸ƒ
        board_stats = {}
        for s in stocks:
            days = s['consecutive_days']
            board_stats[days] = board_stats.get(days, 0) + 1
        logger.info(f"ğŸ“Š è¿æ¿åˆ†å¸ƒ: {board_stats}")
        
        return {
            "code": 200,
            "message": "success",
            "data": {"stocks": stocks},
            "date": data_date
        }
        
    except ImportError:
        logger.error("akshare æœªå®‰è£…ï¼Œä½¿ç”¨å›é€€æ–¹æ¡ˆ")
        return await get_fallback_limit_up(limit)
    except Exception as e:
        logger.error(f"è·å–æ¶¨åœæ± æ•°æ®å¤±è´¥: {e}")
        return await get_fallback_limit_up(limit)


async def get_fallback_limit_up(limit: int) -> Dict[str, Any]:
    """
    å›é€€æ–¹æ¡ˆï¼šä½¿ç”¨ä¸œæ–¹è´¢å¯Œç®€å•æ¥å£è·å–æ¶¨åœè‚¡ç¥¨
    æ³¨æ„ï¼šæ­¤æ–¹æ³•æ— æ³•è·å–å‡†ç¡®çš„è¿æ¿æ•°
    """
    import aiohttp
    
    logger.warning("âš ï¸ ä½¿ç”¨å›é€€æ–¹æ¡ˆè·å–æ¶¨åœæ•°æ®ï¼ˆæ— è¿æ¿æ•°ï¼‰")
    
    url = 'http://push2.eastmoney.com/api/qt/clist/get'
    params = {
        'pn': '1', 'pz': '100', 'po': '1', 'np': '1',
        'ut': 'bd1d9ddb04089700cf9c27f6f7426281',
        'fltt': '2', 'invt': '2', 'fid': 'f3',
        'fs': 'm:0+t:6,m:0+t:13,m:0+t:80,m:1+t:2,m:1+t:23',
        'fields': 'f1,f2,f3,f4,f5,f6,f12,f14'
    }
    
    timeout = aiohttp.ClientTimeout(total=5)
    try:
        async with aiohttp.ClientSession(timeout=timeout) as session:
            async with session.get(url, params=params) as response:
                stocks = []
                if response.status == 200:
                    data = await response.json()
                    if 'data' in data and 'diff' in data['data']:
                        for s in data['data']['diff']:
                            change_pct = float(s.get('f3', 0) or 0)
                            if change_pct > 9.0:  # æ¶¨å¹…å¤§äº9%è§†ä¸ºæ¶¨åœ
                                stocks.append({
                                    "symbol": s.get('f12'),
                                    "code": s.get('f12'),
                                    "name": s.get('f14'),
                                    "price": s.get('f2'),
                                    "change_percent": change_pct,
                                    "consecutive_days": 1,  # å›é€€æ–¹æ¡ˆæ— æ³•è·å–çœŸå®è¿æ¿æ•°
                                    "reason": "æ¶¨åœï¼ˆè¿æ¿æ•°æœªçŸ¥ï¼‰",
                                    "data_source": "eastmoney_fallback"
                                })
                return {
                    "code": 200, 
                    "message": "success (fallback)", 
                    "data": {"stocks": stocks[:limit]},
                    "date": None
                }
    except Exception as e:
        logger.error(f"å›é€€æ–¹æ¡ˆä¹Ÿå¤±è´¥: {e}")
        return {"code": 200, "message": "error", "data": {"stocks": []}}


# =============================================================================
# å†…éƒ¨å¸®åŠ©å‡½æ•°ï¼šè·å–æ¶¨åœæ± DataFrameï¼ˆå¸¦ç¼“å­˜ï¼‰
# =============================================================================
_zt_pool_cache = {"df": None, "date": None, "timestamp": None}

async def _get_zt_pool_df():
    """è·å–æ¶¨åœæ± DataFrameï¼ˆå¸¦å†…å­˜ç¼“å­˜ï¼‰"""
    import akshare as ak
    
    # ç®€å•çš„å†…å­˜ç¼“å­˜ï¼ˆ60ç§’æœ‰æ•ˆï¼‰
    now = datetime.now()
    if (_zt_pool_cache["df"] is not None and 
        _zt_pool_cache["timestamp"] and 
        (now - _zt_pool_cache["timestamp"]).seconds < 60):
        return _zt_pool_cache["df"], _zt_pool_cache["date"]
    
    loop = asyncio.get_event_loop()
    
    for days_ago in range(5):
        try_date = now - timedelta(days=days_ago)
        if try_date.weekday() >= 5:
            continue
        
        date_str = try_date.strftime("%Y%m%d")
        try:
            df = await loop.run_in_executor(None, ak.stock_zt_pool_em, date_str)
            if df is not None and not df.empty:
                _zt_pool_cache["df"] = df
                _zt_pool_cache["date"] = date_str
                _zt_pool_cache["timestamp"] = now
                logger.info(f"âœ… ç¼“å­˜æ¶¨åœæ± æ•°æ®: {len(df)}åª, æ—¥æœŸ: {date_str}")
                return df, date_str
        except Exception as e:
            logger.debug(f"è·å– {date_str} æ¶¨åœæ•°æ®å¤±è´¥: {e}")
            continue
    
    return None, None


# =============================================================================
# äºŒæ¿å€™é€‰æ¥å£
# =============================================================================
@router.get("/second-board-candidates")
async def get_second_board_candidates(limit: int = 20):
    """
    è·å–äºŒæ¿å€™é€‰è‚¡ç¥¨
    
    ä»é¦–æ¿è‚¡ç¥¨ä¸­ç­›é€‰æ˜æ—¥å¯èƒ½æ¶¨åœçš„å€™é€‰ï¼š
    - ç­›é€‰è¿æ¿æ•°=1çš„é¦–æ¿è‚¡ç¥¨
    - æ’é™¤ä¸€å­—æ¿
    - æ’é™¤å¼±åŠ¿è‚¡ï¼ˆç‚¸æ¿è¿‡å¤šã€æ¢æ‰‹ç‡ä½ã€æˆäº¤é¢ä½ï¼‰
    - è®¡ç®—æ™‹çº§æ¦‚ç‡
    """
    try:
        df, data_date = await _get_zt_pool_df()
        
        if df is None or df.empty:
            return {
                "code": 200,
                "message": "æš‚æ— æ¶¨åœæ•°æ®",
                "data": {"candidates": [], "total_count": 0, "update_time": datetime.now().isoformat()}
            }
        
        # ç­›é€‰é¦–æ¿è‚¡ç¥¨ï¼ˆè¿æ¿æ•° == 1ï¼‰
        if 'è¿æ¿æ•°' in df.columns:
            first_board_df = df[df['è¿æ¿æ•°'] == 1].copy()
            logger.info(f"âœ… ç­›é€‰å‡º {len(first_board_df)} åªé¦–æ¿è‚¡ç¥¨ï¼ˆå…± {len(df)} åªæ¶¨åœï¼‰")
        else:
            first_board_df = df.head(30).copy()
            logger.warning("âš ï¸ æ•°æ®ä¸­æ²¡æœ‰'è¿æ¿æ•°'å­—æ®µï¼Œè¿”å›å‰30åª")
        
        # æ’é™¤ä¸€å­—æ¿
        if 'ç‚¸æ¿æ¬¡æ•°' in first_board_df.columns and 'é¦–æ¬¡å°æ¿æ—¶é—´' in first_board_df.columns:
            before_filter = len(first_board_df)
            
            def is_yi_zi_ban(row):
                seal_time = str(row.get('é¦–æ¬¡å°æ¿æ—¶é—´', ''))
                burst_count = int(row.get('ç‚¸æ¿æ¬¡æ•°', 0) or 0)
                if seal_time and len(seal_time) >= 4:
                    if seal_time[:4] <= '0925' and burst_count == 0:
                        return True
                return False
            
            first_board_df = first_board_df[~first_board_df.apply(is_yi_zi_ban, axis=1)]
            logger.info(f"âœ… æ’é™¤ä¸€å­—æ¿åå‰©ä½™ {len(first_board_df)} åªï¼ˆè¿‡æ»¤ {before_filter - len(first_board_df)} åªï¼‰")
        
        candidates = []
        
        for _, row in first_board_df.iterrows():
            code = str(row.get('ä»£ç ', ''))
            name = str(row.get('åç§°', ''))
            change_percent = float(row.get('æ¶¨è·Œå¹…', 0) or 0)
            current_price = float(row.get('æœ€æ–°ä»·', 0) or 0)
            turnover_rate = float(row.get('æ¢æ‰‹ç‡', 0) or 0)
            amount = float(row.get('æˆäº¤é¢', 0) or 0)
            seal_time = str(row.get('é¦–æ¬¡å°æ¿æ—¶é—´', '09:30') or '09:30')
            burst_count = int(row.get('ç‚¸æ¿æ¬¡æ•°', 0) or 0)
            industry = str(row.get('æ‰€å±è¡Œä¸š', 'æœªçŸ¥') or 'æœªçŸ¥')
            
            # è¿‡æ»¤å¼±åŠ¿è‚¡
            if burst_count > 3:
                continue
            if turnover_rate < 5:
                continue
            if amount < 100000000:  # æˆäº¤é¢ < 1äº¿
                continue
            
            # è®¡ç®—æ™‹çº§æ¦‚ç‡
            probability = min(95, int(
                turnover_rate * 3 + 
                min(amount / 1e8, 10) * 5 + 
                change_percent * 2
            ))
            
            candidates.append({
                'code': code,
                'name': name,
                'firstBoardTime': seal_time,
                'sealAmount': round(amount / 1e8, 2),
                'probability': probability,
                'reason': f'é¦–æ¿æ¶¨åœï¼›æ¢æ‰‹ç‡{turnover_rate:.1f}%ï¼›ç‚¸æ¿{burst_count}æ¬¡',
                'risks': ['é¦–æ¿è‚¡ç¥¨ï¼Œå…³æ³¨å°æ¿å¼ºåº¦'],
                'theme': industry,
                'technicalScore': min(100, int(turnover_rate * 5)),
                'marketScore': probability,
                'fundScore': min(100, int((amount / 1e8) * 10)),
                'currentPrice': current_price,
                'changePercent': change_percent,
                'turnoverRate': turnover_rate,
                'burstCount': burst_count
            })
            
            if len(candidates) >= limit:
                break
        
        # æŒ‰æ¦‚ç‡æ’åº
        candidates.sort(key=lambda x: x['probability'], reverse=True)
        
        logger.info(f"âœ… è¿”å› {len(candidates)} åªäºŒæ¿å€™é€‰")
        
        return {
            "code": 200,
            "message": "success",
            "data": {
                "candidates": candidates,
                "total_count": len(candidates),
                "update_time": datetime.now().isoformat(),
                "date": data_date
            }
        }
        
    except ImportError:
        logger.error("akshare æœªå®‰è£…")
        return {"code": 500, "message": "akshareæœªå®‰è£…", "data": {"candidates": []}}
    except Exception as e:
        logger.error(f"è·å–äºŒæ¿å€™é€‰å¤±è´¥: {e}")
        return {"code": 500, "message": str(e), "data": {"candidates": []}}


# =============================================================================
# å®æ—¶é¢„æµ‹æ¥å£ï¼ˆç›¯ç›˜é›·è¾¾ç”¨ï¼‰
# =============================================================================
@router.get("/realtime-predictions")
async def get_realtime_predictions(limit: int = 50):
    """
    å®æ—¶æ¶¨åœé¢„æµ‹ï¼ˆç›¯ç›˜é›·è¾¾ï¼‰
    
    è¿”å›åˆ†æ—¶æ®µçš„æ¶¨åœé¢„æµ‹æ•°æ®ï¼Œç”¨äºç›¯ç›˜é›·è¾¾ç»„ä»¶
    """
    try:
        df, data_date = await _get_zt_pool_df()
        
        if df is None or df.empty:
            return {
                "code": 200,
                "message": "æš‚æ— æ¶¨åœæ•°æ®",
                "data": {
                    "segments": [],
                    "statistics": {"total_stocks": 0},
                    "update_time": datetime.now().isoformat()
                }
            }
        
        # æ—¶é—´æ®µå®šä¹‰ - IDä¸å‰ç«¯activeTabåŒ¹é…
        time_segments = [
            {"id": "auction", "name": "ğŸš€ å¼€ç›˜å†²åˆº", "period": "09:30-10:00", "description": "å¼€ç›˜30åˆ†é’Ÿå†…æ¶¨åœ"},
            {"id": "anomaly", "name": "ğŸ“ˆ æ—©ç›˜ä¸»å‡", "period": "10:00-11:00", "description": "æ—©ç›˜ä¸»å‡é˜¶æ®µ"},
            {"id": "breakthrough", "name": "ğŸ”„ åˆç›˜å‘åŠ›", "period": "11:00-13:30", "description": "åˆç›˜å‰å"},
            {"id": "late", "name": "âš¡ å°¾ç›˜çªè¢­", "period": "13:30-15:00", "description": "å°¾ç›˜æ‹‰å‡"}
        ]

        
        # æŒ‰å°æ¿æ—¶é—´åˆ†ç±»
        segmented_stocks = {i: [] for i in range(len(time_segments))}
        
        for _, row in df.head(100).iterrows():
            code = str(row.get('ä»£ç ', ''))
            name = str(row.get('åç§°', ''))
            change_percent = float(row.get('æ¶¨è·Œå¹…', 0) or 0)
            price = float(row.get('æœ€æ–°ä»·', 0) or 0)
            turnover_rate = float(row.get('æ¢æ‰‹ç‡', 0) or 0)
            amount = float(row.get('æˆäº¤é¢', 0) or 0)
            seal_time = str(row.get('é¦–æ¬¡å°æ¿æ—¶é—´', '') or '')
            consecutive_days = int(row.get('è¿æ¿æ•°', 1) or 1)
            
            # è®¡ç®—é¢„æµ‹åˆ†æ•°
            score = min(100, change_percent * 8 + turnover_rate * 2 + min(amount / 1e7, 10) * 5)
            
            if score >= 85:
                level = "æé«˜"
            elif score >= 75:
                level = "é«˜"
            elif score >= 65:
                level = "ä¸­é«˜"
            else:
                level = "ä¸­"
            
            stock_data = {
                "code": code,
                "name": name,
                "price": price,
                "changePercent": change_percent,
                "turnoverRate": turnover_rate,
                "amount": amount,
                "predictionScore": round(score, 1),
                "predictionLevel": level,
                "sealTime": seal_time,
                "consecutive_days": consecutive_days,
                "predictionReasons": [
                    f"æ¶¨å¹…{change_percent:.2f}%",
                    f"æ¢æ‰‹{turnover_rate:.2f}%",
                    f"{consecutive_days}è¿æ¿" if consecutive_days > 1 else "é¦–æ¿"
                ]
            }
            
            # æ ¹æ®å°æ¿æ—¶é—´åˆ†ç±»
            segment_id = 0
            if seal_time and len(seal_time) >= 4:
                hhmm = seal_time[:4].replace(':', '')
                if hhmm <= '1000':
                    segment_id = 0
                elif hhmm <= '1100':
                    segment_id = 1
                elif hhmm <= '1330':
                    segment_id = 2
                else:
                    segment_id = 3
            
            segmented_stocks[segment_id].append(stock_data)
        
        # æ„å»ºè¿”å›æ•°æ®
        result_segments = []
        total_stocks = 0
        
        for i, segment_info in enumerate(time_segments):
            stocks = segmented_stocks[i][:limit]
            stocks.sort(key=lambda x: x['predictionScore'], reverse=True)
            total_stocks += len(stocks)
            
            # å‰ç«¯æœŸæœ›çš„æ ¼å¼: {id, name, description, stocks, count}
            result_segments.append({
                "id": str(segment_info["id"]),  # å‰ç«¯ç”¨å­—ç¬¦ä¸²IDåŒ¹é…tab
                "name": segment_info["name"],
                "description": segment_info["description"],
                "stocks": stocks,
                "count": len(stocks)
            })

        
        return {
            "code": 200,
            "message": "success",
            "data": {
                "segments": result_segments,
                "statistics": {
                    "total_stocks": total_stocks,
                    "segments_count": len(time_segments)
                },
                "update_time": datetime.now().isoformat(),
                "date": data_date
            }
        }
        
    except ImportError:
        logger.error("akshare æœªå®‰è£…")
        return {"code": 500, "message": "akshareæœªå®‰è£…", "data": {"segments": []}}
    except Exception as e:
        logger.error(f"è·å–å®æ—¶é¢„æµ‹å¤±è´¥: {e}")
        return {"code": 500, "message": str(e), "data": {"segments": []}}

